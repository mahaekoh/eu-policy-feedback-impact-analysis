# EU Policy Feedback Impact Analysis

A data pipeline for studying whether public consultation feedback influences EU policy documents published through the European Commission's ["Have Your Say"](https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives_en) portal.

## Research Question

When the European Commission publishes a draft regulation, directive, or impact assessment and opens it for public feedback, **does that feedback measurably influence the documents the Commission publishes afterward?**

This project collects the full documentary record needed to investigate that question: every document the Commission published before and after receiving feedback, every piece of feedback submitted (including attached files in any language), and AI-generated summaries to support analysis at scale.

## Methodology

The analysis follows a documentary comparison approach:

1. **Collect** all EU "Have Your Say" initiatives with feedback periods between December 2019 and November 2024 (1,825 initiatives)
2. **Extract** the full text of every published document and every feedback attachment (PDFs, Word documents, RTF, ODT, plain text)
3. **Recover** text from scanned or image-based PDFs using optical character recognition (OCR)
4. **Translate** non-English feedback attachments to English using a large language model
5. **Identify** the temporal boundary: which documents were published *before* feedback was received, and which came *after*
6. **Summarize** long documents and feedback attachments using AI to enable qualitative comparison at scale

A subset of 145 initiatives was selected for detailed before/after analysis. Of these, 128 had documents published after feedback was received, making them candidates for studying feedback influence.

## Data Source

All data comes from the European Commission's [Better Regulation](https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives_en) portal. The portal publishes:

- **Initiative metadata**: title, type of act, department, policy area, status
- **Publications**: documents at each stage of the policy process (planned initiative, call for evidence, public consultation, draft act, adoption)
- **Feedback**: comments submitted by citizens, companies, business associations, NGOs, trade unions, academic institutions, and public authorities, along with any attached documents

## What the Pipeline Produces

### Master initiative list (`eu_initiatives.csv`)

A CSV file with all 1,825 initiatives, including their ID, title, type of act, feedback dates, policy topics, and URL.

### Per-initiative detail files (`initiative_details/*.json`)

One JSON file per initiative (1,785 files) containing the complete record: all publications with their documents (full extracted text), all feedback items with their metadata (date, language, respondent type, country, organization) and attached files (full extracted text, translated to English where needed).

### Before/after analysis files (`before_after_analysis_v2/*.json`)

One JSON file per analysed initiative (128 files) with the data restructured for comparison:

- **`documents_before_feedback`** — full text of all documents published up to and including the first publication that received feedback
- **`documents_after_feedback`** — full text of documents from the final publication
- **`middle_feedback`** — all feedback submitted between the first feedback publication and the final document publication

### AI-generated summaries

Each document and feedback attachment in the before/after analysis can be enriched with a `summary` field: a detailed summary (up to 10 paragraphs) generated by a 120-billion-parameter language model.

## Data Structure

Each initiative JSON file follows this hierarchy:

```
Initiative
  |-- id, title, type of act, department, topics
  |-- Publications (ordered by date)
       |-- publication type (e.g. "Call for evidence", "Draft act")
       |-- published date, feedback end date
       |-- Documents
       |    |-- filename, label, extracted text, page count
       |-- Feedback items
            |-- date, language, respondent type, country, organization
            |-- feedback text (free-text comment)
            |-- Attachments
                 |-- filename, extracted text (translated if needed)
```

### Respondent types in the data

Feedback comes from a range of respondent types as classified by the EU portal:

- `EU_CITIZEN` — individual EU citizens
- `COMPANY` — individual companies
- `BUSINESS_ASSOCIATION` — industry and trade associations
- `NGO` — non-governmental organizations
- `TRADE_UNION` — labour unions
- `ACADEMIC_RESEARCH` — academic and research institutions
- `PUBLIC_AUTHORITY` — national or regional government bodies
- `CONSUMER_ORGANISATION` — consumer advocacy groups
- `ENVIRONMENTAL_ORGANISATION` — environmental advocacy groups
- `OTHER` — other organisations

## Key Numbers

| Metric | Count |
|--------|-------|
| Total initiatives scraped | 1,825 |
| Initiatives with full detail data | 1,785 |
| Initiatives selected for analysis | 145 |
| Initiatives with documents after feedback | 128 |
| Initiatives with no Commission response after feedback | 19 |

## Scope and Limitations

- **Sampling**: 145 of 1,825 initiatives were selected for the before/after analysis (~8%). The selection criteria and rationale should be documented separately for any publication.
- **Correlation, not causation**: Finding that a document changed after feedback does not prove the feedback caused the change. The Commission may have planned revisions independently.
- **Text extraction quality**: Most PDFs extract cleanly, but some scanned documents required OCR, which can introduce errors. Original text is preserved alongside OCR results for verification.
- **Translation quality**: Non-English feedback was translated by a large language model. Translations are generally accurate but may miss nuance. Original text is preserved alongside translations.
- **Feedback text vs. attachments**: Some respondents submit detailed positions as attached documents rather than in the free-text comment field. The pipeline captures both, but analysis should account for this variation.
- **Time period**: Feedback periods from December 2019 to November 2024.

## Working with the Data

The output files are standard JSON. You can explore them with:

- **Python**: `json.load()` to read, then navigate the nested structure
- **jq** (command line): e.g. `jq '.publications[0].feedback | length' initiative_details/12970.json` to count feedback items
- **Any JSON viewer**: browser extensions, VS Code, or online tools like [jsoncrack.com](https://jsoncrack.com)

## Configuration Files

- **`initiative-whitelist-145.txt`** — list of 145 initiative IDs selected for detailed analysis (one ID per line)

## Technical Details

For a complete technical breakdown of each pipeline component, dependencies, data schemas, and instructions for running the pipeline, see [ARCHITECTURE.md](ARCHITECTURE.md).
